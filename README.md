# Bachelor Thesis: Input or Output Space Alignment? A comparative study for continual unsupervisded domain adaptation in semantic image segmentation

## Abstract
Developing fully autonomous driving vehicles is a key ambition of all major automotive producers for the current decade. One of the biggest challenges to achieve this goal is the precise and reliable identification of objects in front and around the vehicle, e.g. streets, cars, and people. Semantic Image Segmentation can provide excellent results for this task but requires a high effort for manually labeling the required training data, i.e. identifying all relevant objects in several thousand images. As the quality of semantic image segmentation decreases significantly when the environment changes (e.g. different weather conditions), this task of labeling data would have to be repeated for many different environments, thus multiplying the manual effort. 

The reason for the performance degradation is the distribution shift in the data between the environment for which the segmentation network was trained (labeled data) and the environment it is applied in. Unsupervised domain adaptation (UDA) can solve this problem. Requiring only labeled training data from one source environment and unlabeled data from a different, so-called, target environment, UDA helps to overcome the distribution shift and improves segmentation performance for the target environment. However, for use in autonomous driving, adapting to one target environment is not sufficient. Rather, good performance is required for multiple target environments, for which the unlabeled training data generally becomes available sequentially. Continual learning (CL) allows the segmentation model to adapt to new target environments without forgetting information about previous ones. The combined application of both concepts -- UDA and CL -- is called continual unsupervised domain adaptation (continual UDA).

In this work, we compare five frameworks for continual UDA. Three of them use style transfer to align data distributions in the input space (CACE, $\textrm{C-WCT}^2$, C-CMD). The other two align distributions in the output space using adversarial learning (ETM, MuHDi). The first style-transfer-based method is CACE, an already established framework for continual UDA. Keeping the basic structure of CACE, but integrating two different style transfer algorithms (i.e. $\textrm{WCT}^2$ and CMD) that are expected to overcome some limitations of the CACE style transfer algorithm (e.g. they preserve more details, thus being more photorealistic), we created two new frameworks for style-transfer-based continual UDA. The last two frameworks ETM and MuHDi, which align distributions in output space, mainly diverge in the way they facilitate continual learning.

Our experiments delivered two results. (1) Even though using better style transfer algorithms, the two new frameworks $\textrm{C-WCT}^2$ and C-CMD did not achieve better performance than the established CACE framework. As both $\textrm{C-WCT}^2$  and C-CMD additionally suffer from limitations in real-world application, we consider CACE the superior method. (2) Input-space alignment (CACE) delivered superior results than output-space alignment (ETM, MuHDi). In particular, both output-space-aligned frameworks have difficulties with continual learning and are not able to sufficiently prevent forgetting when adapting to new environments.

## Repository Structure
- [Thesis](./Thesis.pdf)
- [Presentation](./Presentation_Tim_Lindenau.pdf)
